{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0960402-ef26-4a97-88ed-89642ffbc2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Standardization, or mean removal and variance scaling\n",
    "### Standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler\n",
    "\n",
    "scaler.mean_\n",
    "\n",
    "scaler.scale_\n",
    "\n",
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled\n",
    "\n",
    "X_scaled.mean(axis=0)\n",
    "\n",
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01538c73-6748-4710-bcf1-67d5751be9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scalers, Transformers, and Normalizers compute the mean and standard deviation on a training set \n",
    "## so as to be able to later re-apply the same transformation on the testing set. \n",
    "\n",
    "\n",
    "# This class is hence suitable for use in the early steps of a Pipeline:\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "\n",
    "pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c12962f-cffc-41af-abe9-b5e3d681d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Scaling features to a range\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e8b4f4-03af-4679-ac4a-b6149235b6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same scaling and shifting operations will be applied to be consistent \n",
    "# with the transformation performed on the train data:\n",
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06073b9-ca64-4fae-8e10-a55cec29c943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It is possible to introspect the scaler attributes \n",
    "# to find about the exact nature of the transformation learned on the training data:\n",
    "min_max_scaler.scale_\n",
    "\n",
    "min_max_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022fdb27-c19c-4a18-ad45-256422a35410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax Standard Deviation of X:\n",
      "[[0.12381866 0.52439337 0.4864274  ... 0.49285474 0.67906612 0.21207425]\n",
      " [0.84375859 0.64635665 0.5435557  ... 0.82812533 0.64916728 0.42922402]\n",
      " [0.55742129 0.70255081 0.81972143 ... 0.79152641 0.42087826 0.34748043]\n",
      " ...\n",
      " [0.51666315 0.20845695 0.60900112 ... 0.49782028 0.43780975 0.39628525]\n",
      " [0.73933131 0.7513872  0.46276529 ... 0.49859413 0.28347444 0.24459206]\n",
      " [0.52328794 0.77176729 0.53917657 ... 0.75291143 0.58023005 0.34009484]]\n",
      "\n",
      "MinMaxed X:\n",
      "[[-2.02514259  0.0291022  -0.47494531 ... -0.33450124  0.86575519\n",
      "  -1.20029641]\n",
      " [ 1.61371127  0.65992405 -0.15005559 ...  1.37570681  0.70117274\n",
      "  -0.2975635 ]\n",
      " [ 0.16645221  0.95057302  1.42050425 ...  1.18901653 -0.55547712\n",
      "  -0.63738713]\n",
      " ...\n",
      " [-0.03955515 -1.60499282  0.22213377 ... -0.30917212 -0.46227529\n",
      "  -0.43449623]\n",
      " [ 1.08589557  1.2031659  -0.6095122  ... -0.3052247  -1.31183623\n",
      "  -1.06511366]\n",
      " [-0.00607091  1.30857636 -0.17495976 ...  0.99204235  0.32169781\n",
      "  -0.66809045]]\n"
     ]
    }
   ],
   "source": [
    "# If MinMaxScaler is given an explicit feature_range=(min, max) the full formula is:\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "X_scaled = X_std * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0)\n",
    "print(f\"MinMax Standard Deviation of X:\\n{X_std}\\n\\nMinMaxed X:\\n{X_scaled}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95247760-0c03-4399-9106-4a4eac6f3a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MaxAbsScaler works in a very similar fashion, \n",
    "## but scales in a way that the training data lies within the range [-1, 1] \n",
    "## by dividing through the largest maximum value in each feature. \n",
    "## It is meant for data that is already centered at zero or sparse data.\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs\n",
    "max_abs_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafe928-97b2-4e07-b3c3-9eea851f995a",
   "metadata": {},
   "source": [
    "# Scaling sparse data\n",
    "Centering sparse data would destroy the sparseness structure in the data, \n",
    "and thus rarely is a sensible thing to do. \n",
    "However, it can make sense to scale sparse inputs, \n",
    "especially if features are on different scales.\n",
    "\n",
    "MaxAbsScaler was specifically designed for scaling sparse data, \n",
    "and is the recommended way to go about this. \n",
    "However, StandardScaler can accept scipy.sparse matrices as input, \n",
    "as long as with_mean=False is explicitly passed to the constructor.\n",
    "\n",
    "### RobustScaler cannot be fitted to sparse inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scaling data with outliers\n",
    "Scaling using the mean and variance of the data is likely to not work very well. \n",
    "In these cases, you can use RobustScaler as a drop-in replacement instead.\n",
    "\n",
    "\n",
    "\n",
    "# Scaling vs Whitening\n",
    "\n",
    "It is sometimes not enough to center and scale the features independently, \n",
    "since a downstream model can further make some assumption \n",
    "on the linear independence of the features.\n",
    "To address this issue you can use PCA \n",
    "with whiten=True to further remove the linear correlation across features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Centering kernel matrices\n",
    "If you have a kernel matrix of a kernel K\n",
    "that computes a dot product in a feature space (possibly implicitly) \n",
    "a KernelCenterer can transform the kernel matrix \n",
    "so that it contains inner products in the feature space\n",
    "followed by the removal of the mean in that space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b430442-d777-441f-b97a-e487118b9131",
   "metadata": {},
   "source": [
    "# Non-linear transformation\n",
    "\n",
    "Mapping to a Uniform distribution-basic quantile transformation\n",
    "QuantileTransformer provides a non-parametric transformation \n",
    "to map the data to a uniform distribution with values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca87fa3e-c166-4f15-a319-78f6c41e5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:2762: UserWarning: n_quantiles (1000) is greater than the total number of samples (112). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01351351, 0.25      , 0.47747748, 0.60472973, 0.94144144])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "\n",
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) # Raw data\n",
    "np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100]) # Approximated to Quantile!\n",
    "\n",
    "np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n",
    "np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6b939-58b2-4403-ac31-146c019ef714",
   "metadata": {},
   "source": [
    "# Mapping to a Gaussian distribution\n",
    "Power transforms are a family of parametric, monotonic transformations \n",
    "that aim to map data from any distribution \n",
    "to as close to a Gaussian distribution as possible \n",
    "in order to stabilize variance and minimize skewness.\n",
    "\n",
    "## PowerTransformer currently provides two such power transformations, \n",
    "the Yeo-Johnson transform and the Box-Cox transform.\n",
    "Box-Cox can only be applied to strictly positive data. \n",
    "In both methods, the transformation is parameterized by lambda,\n",
    "which is determined through maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372a08d2-a683-4673-9db3-446ad0939a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49024349,  0.17881995, -0.1563781 ],\n",
       "       [-0.05102892,  0.58863195, -0.57612415],\n",
       "       [ 0.69420009, -0.84857823,  0.10051454]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n",
    "X_lognormal # Only has positive values\n",
    "pt.fit_transform(X_lognormal) # Now there are negative values also!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8074d62e-8555-4a6b-952a-3728e419a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:2762: UserWarning: n_quantiles (1000) is greater than the total number of samples (150). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.3, 2. , 1. , 0.1],\n",
       "       [4.4, 2.2, 1.1, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.5, 2.3, 1.3, 0.1],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.7, 0.4],\n",
       "       [5.3, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.9, 0.5],\n",
       "       [5.4, 2.9, 1.9, 0.6],\n",
       "       [5.4, 2.9, 3. , 1. ],\n",
       "       [5.4, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.6, 1. ],\n",
       "       [5.5, 2.9, 3.7, 1. ],\n",
       "       [5.5, 3. , 3.8, 1.1],\n",
       "       [5.5, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.7, 3. , 4. , 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.4],\n",
       "       [5.8, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.2, 4.7, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.3, 5. , 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.6, 3.4, 5.4, 1.9],\n",
       "       [6.6, 3.4, 5.4, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.9, 3.6, 5.7, 2.2],\n",
       "       [6.9, 3.6, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.3],\n",
       "       [7. , 3.7, 5.9, 2.3],\n",
       "       [7.1, 3.8, 5.9, 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6.1, 2.3],\n",
       "       [7.3, 3.8, 6.1, 2.3],\n",
       "       [7.4, 3.8, 6.1, 2.3],\n",
       "       [7.6, 3.9, 6.3, 2.4],\n",
       "       [7.7, 3.9, 6.4, 2.4],\n",
       "       [7.7, 4. , 6.6, 2.4],\n",
       "       [7.7, 4.1, 6.7, 2.5],\n",
       "       [7.7, 4.2, 6.7, 2.5],\n",
       "       [7.9, 4.4, 6.9, 2.5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To map data to a normal distribution using QuantileTransformer,\n",
    "## set output_distribution='normal'. \n",
    "quantile_transformer = preprocessing.QuantileTransformer(\n",
    "    output_distribution='normal', random_state=0)\n",
    "X_trans = quantile_transformer.fit_transform(X)\n",
    "quantile_transformer.quantiles_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce991d9-09ee-43c9-a393-92a6d77d2dd5",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "Normalization is the process of scaling individual samples to have unit norm. \n",
    "This process can be useful if you plan to use a quadratic form \n",
    "such as the dot-product or any other kernel \n",
    "to quantify the similarity of any pair of samples.\n",
    "\n",
    "This assumption is the base of the Vector Space Model \n",
    "often used in text classification and clustering contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7595d636-d2fe-4ca9-b34f-c7fcb7b97b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0b4878-983b-4cad-a8ae-5def3a730add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### a utility class Normalizer that implements the same operation \n",
    "## using the Transformer API (even though the fit method is useless in this case: \n",
    "## the class is stateless as this operation treats samples independently).\n",
    "normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n",
    "normalizer\n",
    "\n",
    "normalizer.transform(X)\n",
    "\n",
    "normalizer.transform([[-1.,  1., 0.]])\n",
    "## This class is suitable for use in the early steps of a Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422ff19-c461-43d4-971a-948d9a30e009",
   "metadata": {},
   "source": [
    "normalize and Normalizer accept both dense array-like and sparse matrices \n",
    "from scipy.sparse as input.\n",
    "For sparse input the data is converted to the Compressed Sparse Rows \n",
    "(see scipy.sparse.csr_matrix) before being fed to efficient Cython routines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea03bfb-cd7c-4df6-8f25-fcb8350326a3",
   "metadata": {},
   "source": [
    "# Encoding categorical features \n",
    "To convert categorical features to such integer codes, \n",
    "we can use the OrdinalEncoder. \n",
    "This estimator transforms each categorical feature \n",
    "to one new feature of integers (0 to n_categories - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349f5873-999b-4a2d-92ec-2660d47982ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "enc.transform([['female', 'from US', 'uses Safari']])\n",
    "# these expect continuous input, \n",
    "# and would interpret the categories as being ordered, which is often not desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09193d30-f31d-4ccc-ba69-f39dd15d9325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, OrdinalEncoder will also passthrough missing values \n",
    "# that are indicated by np.nan.\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fb0706e-a192-4df5-a3ab-4a85f89138e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OrdinalEncoder provides a parameter encoded_missing_value=\n",
    "# to encode the missing values without the need to create a pipeline \n",
    "# and using SimpleImputer.\n",
    "enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "enc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3afebc6-e01e-40c5-869a-138d31e1a9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Above codes are equivalent to below.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "enc = Pipeline(steps=[\n",
    "    (\"encoder\", preprocessing.OrdinalEncoder()),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "])\n",
    "enc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1b39b-b6db-4a0c-b9e7-14e2e41e7c6c",
   "metadata": {},
   "source": [
    "# The OneHotEncoder \n",
    "which transforms each categorical feature \n",
    "with n_categories possible values into n_categories binary features, \n",
    "with one of them 1, and all others 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bb015-ba33-421a-a9cf-41b12dfa682f",
   "metadata": {},
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Safari']]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed91004e-4842-4722-b105-08b2b893d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['female', 'male'], dtype=object), array(['from Africa', 'from Asia', 'from Europe', 'from US'], dtype=object), array(['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari'],\n",
      "      dtype=object)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## By default, the values each feature can take \n",
    "## is inferred automatically from the dataset \n",
    "### and can be found in the categories_ attribute:\n",
    "print(enc.categories_)\n",
    "\n",
    "#### It is possible to specify this explicitly using the parameter categories=\n",
    "genders = ['female', 'male']\n",
    "locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n",
    "browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n",
    "enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n",
    "# Note that for there are missing categorical values for the 2nd and 3rd\n",
    "# feature\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df788045-e9d3-4f17-a178-6e643844d140",
   "metadata": {},
   "source": [
    "If there is a possibility that \n",
    "the training data might have missing categorical features, \n",
    "it can often be better to specify handle_unknown='infrequent_if_exist' \n",
    "instead of setting the categories manually as above. \n",
    "When handle_unknown='infrequent_if_exist' is specified \n",
    "and unknown categories are encountered during transform, \n",
    "no error will be raised but the resulting one-hot encoded columns \n",
    "for this feature will be all zeros \n",
    "or considered as an infrequent category if enabled. \n",
    "#### (handle_unknown='infrequent_if_exist' is only supported for one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbcc793c-cee0-49d2-9e9b-48e0b4c34cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e74987c-6fb5-4ff3-8c21-7c1b7d1d7021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To encode each column into n_categories - 1 columns \n",
    "### instead of n_categories columns by using the drop= parameter. \n",
    "### This parameter allows the user to specify a category \n",
    "### for each feature to be dropped.\n",
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n",
    "drop_enc.categories_\n",
    "drop_enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d94ea2e4-88d5-49ed-a18f-d72fa7fa8ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To drop one of the two columns only for features with 2 categories, \n",
    "### you can set the parameter drop='if_binary'.\n",
    "X = [['male', 'US', 'Safari'],\n",
    "     ['female', 'Europe', 'Firefox'],\n",
    "     ['female', 'Asia', 'Chrome']]\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\n",
    "drop_enc.categories_\n",
    "drop_enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3eac26e9-69d5-4e96-8dd1-04451b85f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:241: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## When handle_unknown='ignore' and drop is not None, \n",
    "## unknown categories will be encoded as all zeros\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='first',\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "drop_enc.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1ff2b2f-b494-4b4b-9489-f1627a914b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:241: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['female', None, None]], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OneHotEncoder.inverse_transform \n",
    "### will map all zeros to the dropped category \n",
    "### if a category is dropped and None if a category is not dropped\n",
    "drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,\n",
    "                                       handle_unknown='ignore').fit(X)\n",
    "X_test = [['unknown', 'America', 'IE']]\n",
    "X_trans = drop_enc.transform(X_test)\n",
    "X_trans\n",
    "drop_enc.inverse_transform(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a07664ae-558b-4a74-bd38-9bcf00f05edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### OneHotEncoder supports categorical features with missing values \n",
    "#### by considering the missing values as an additional category:\n",
    "X = [['male', 'Safari'],\n",
    "     ['female', None],\n",
    "     [np.nan, 'Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55621bca-7435-4abf-9947-733d0eb1c05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## If a feature contains both np.nan and None, \n",
    "## they will be considered separate categories:    \n",
    "X = [['Safari'], [None], [np.nan], ['Firefox']]\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n",
    "enc.categories_\n",
    "enc.transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10165ad-ad7b-48fa-94dd-55349e71b919",
   "metadata": {},
   "source": [
    "min_frequency= is either an integer greater or equal to 1, or a float in the interval (0.0, 1.0). \n",
    "If min_frequency is an integer, categories with a cardinality smaller than min_frequency will be considered infrequent. \n",
    "\n",
    "If min_frequency is a float, categories with a cardinality smaller than this fraction of the total number of samples will be considered infrequent. \n",
    "The default value is 1, which means every category is encoded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "683a3db6-6a85-4a21-9970-632c0c0029d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\n",
    "              ['snake'] * 3], dtype=object).T\n",
    "enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\n",
    "enc.infrequent_categories_\n",
    "enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
